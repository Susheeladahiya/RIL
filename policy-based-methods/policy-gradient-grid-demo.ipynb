{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid world environment\n",
    "GRID_ROWS, GRID_COLS = 4, 5  # 4x5 Grid World\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_space_size = len(actions)\n",
    "action_symbols = ['U', 'D', 'L', 'R']  # For display purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Table:\n",
      "[[   0   -1   -1   -1   -1]\n",
      " [  -1   -1   -1   -1   -1]\n",
      " [  -1   -1 -100   -1   -1]\n",
      " [  -1   -1   -1   -1  100]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid world environment\n",
    "GRID_ROWS, GRID_COLS = 4, 5  # 4x5 Grid World\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_space_size = len(actions)\n",
    "action_symbols = ['U', 'D', 'L', 'R']  # For display purposes\n",
    "\n",
    "# Define the policy network parameters (random initialization)\n",
    "theta = np.random.rand(GRID_ROWS, GRID_COLS, action_space_size)\n",
    "\n",
    "# Define learning rate\n",
    "alpha = 0.1  # Policy learning rate\n",
    "gamma = 0.9  # Discount factor for future rewards\n",
    "\n",
    "# Define the rewards grid\n",
    "rewards = np.zeros((GRID_ROWS, GRID_COLS))\n",
    "rewards[3, 4] = 1  # Goal position\n",
    "\n",
    "\n",
    "# Function to print reward table\n",
    "def print_reward_table():\n",
    "    print(\"Reward Table:\")\n",
    "    print(rewards)\n",
    "    print(\"\\n\")\n",
    "print_reward_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy softmax function to get action probabilities\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Function to choose action based on policy (probabilities from softmax)\n",
    "def choose_action(state):\n",
    "    row, col = state\n",
    "    action_probs = softmax(theta[row, col])\n",
    "    return np.random.choice(len(actions), p=action_probs)\n",
    "\n",
    "# Function to take a step in the grid world\n",
    "def take_step(state, action):\n",
    "    row, col = state\n",
    "\n",
    "    if action == 0:  # Up\n",
    "        new_state = (max(0, row - 1), col)\n",
    "    elif action == 1:  # Down\n",
    "        new_state = (min(GRID_ROWS - 1, row + 1), col)\n",
    "    elif action == 2:  # Left\n",
    "        new_state = (row, max(0, col - 1))\n",
    "    else:  # Right\n",
    "        new_state = (row, min(GRID_COLS - 1, col + 1))\n",
    "\n",
    "    reward = rewards[new_state]\n",
    "    return new_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Table:\n",
      "[[   0   -1   -1   -1   -1]\n",
      " [  -1   -1   -1   -1   -1]\n",
      " [  -1   -1 -100   -1   -1]\n",
      " [  -1   -1   -1   -1  100]]\n",
      "\n",
      "\n",
      "Iteration 10:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L D D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 0.56 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 20:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L D D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 30:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D R      | 1.00 1.00 1.00 1.00 0.93\n",
      "R D L D D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 40:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L D D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 50:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 60:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 0.73 1.00\n",
      "D R R D R      | 1.00 1.00 1.00 1.00 0.95\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 70:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D R      | 1.00 1.00 1.00 1.00 0.89\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.98\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 80:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 90:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 0.97 1.00\n",
      "D R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 100:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 1.00 0.97\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.96\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Final policy parameters (theta):\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 1.00 0.97\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.96\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "Path taken: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (0, 3), (1, 3), (2, 3), (2, 4), (3, 4)]\n",
      "Reward Table:\n",
      "[[   0   -1   -1   -1   -1]\n",
      " [  -1   -1   -1   -1   -1]\n",
      " [  -1   -1 -100   -1   -1]\n",
      " [  -1   -1   -1   -1  100]]\n",
      "\n",
      "\n",
      "Iteration 10:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 20:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 30:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 40:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 0.94 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 50:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 0.88 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.97\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 60:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 0.77 0.99\n",
      "D R R D L      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 70:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 0.94 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 80:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R R R      | 1.00 1.00 1.00 0.87 1.00\n",
      "D R R R L      | 1.00 1.00 1.00 0.99 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 90:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R L      | 1.00 1.00 1.00 0.86 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 0.98\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 100:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R L      | 1.00 1.00 1.00 0.99 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 0.99\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Final policy parameters (theta):\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R L      | 1.00 1.00 1.00 0.99 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 0.99\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "Path taken: [(0, 0), (0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (2, 4), (3, 4)]\n",
      "Reward Table:\n",
      "[[   0   -1   -1   -1   -1]\n",
      " [  -1   -1   -1   -1   -1]\n",
      " [  -1   -1 -100   -1   -1]\n",
      " [  -1   -1   -1   -1  100]]\n",
      "\n",
      "\n",
      "Iteration 10:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 0.99\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 20:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 0.99\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 30:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R U R      | 1.00 1.00 1.00 0.99 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 40:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 50:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 60:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 0.93 1.00\n",
      "R D L R L      | 1.00 1.00 0.30 1.00 0.90\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 70:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 0.99 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 80:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D D      | 1.00 1.00 1.00 0.99 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 90:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 100:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 0.99 1.00\n",
      "D R R D D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Final policy parameters (theta):\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 0.99 1.00\n",
      "D R R D D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "Path taken: [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 1), (0, 2), (0, 2), (0, 3), (1, 3), (2, 3), (2, 2), (1, 2), (1, 3), (2, 3), (2, 4), (2, 4), (3, 4)]\n",
      "Reward Table:\n",
      "[[   0   -1   -1   -1   -1]\n",
      " [  -1   -1   -1   -1   -1]\n",
      " [  -1   -1 -100   -1   -1]\n",
      " [  -1   -1   -1   -1  100]]\n",
      "\n",
      "\n",
      "Iteration 10:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 20:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 30:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 40:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 0.98\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 0.85\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 50:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 0.96 1.00 1.00 1.00 0.98\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 0.99\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 60:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 0.99\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 70:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 0.99\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 80:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 0.98\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.99\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 90:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 0.98\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.98\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 100:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.95\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Final policy parameters (theta):\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.95\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "Path taken: [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (1, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 3), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 4), (2, 3), (1, 3), (1, 4)]\n",
      "Reward Table:\n",
      "[[   0   -1   -1   -1   -1]\n",
      " [  -1   -1   -1   -1   -1]\n",
      " [  -1   -1 -100   -1   -1]\n",
      " [  -1   -1   -1   -1  100]]\n",
      "\n",
      "\n",
      "Iteration 10:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 20:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 30:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 40:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 50:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.97\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 60:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.99\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 70:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R R D      | 1.00 1.00 1.00 1.00 0.93\n",
      "R D L R R      | 1.00 1.00 0.30 1.00 0.99\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 80:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 1.00 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 0.98 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 90:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R R D R      | 1.00 1.00 1.00 0.98 1.00\n",
      "D R R D L      | 1.00 1.00 1.00 0.99 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Iteration 100:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "R R R D R      | 0.99 1.00 1.00 0.99 1.00\n",
      "D R R R L      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "\n",
      "\n",
      "Final policy parameters (theta):\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "R R R D R      | 0.99 1.00 1.00 0.99 1.00\n",
      "D R R R L      | 1.00 1.00 1.00 1.00 1.00\n",
      "R D L R D      | 1.00 1.00 0.30 1.00 1.00\n",
      "L L D R D      | 1.00 1.00 1.00 1.00 0.30\n",
      "Path taken: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (0, 4), (0, 4), (0, 4), (0, 4), (0, 4), (0, 4), (0, 4), (0, 4), (0, 4), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3), (1, 4), (1, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Policy Gradient Update\n",
    "def policy_gradient_update(state, action, G):\n",
    "    row, col = state\n",
    "    action_probs = softmax(theta[row, col])\n",
    "    \n",
    "    # Gradient ascent update (θ ← θ + α * G * ∇logπ)\n",
    "    for a in range(action_space_size):\n",
    "        grad_log_pi = (1 if a == action else 0) - action_probs[a]  # ∇logπ for each action\n",
    "        theta[row, col, a] += alpha * G * grad_log_pi\n",
    "\n",
    "# Function to display policy and probabilities side-by-side\n",
    "def display_policy():\n",
    "    direction_grid = np.full((GRID_ROWS, GRID_COLS), '', dtype='<U1')\n",
    "    probability_grid = np.zeros((GRID_ROWS, GRID_COLS))\n",
    "\n",
    "    for row in range(GRID_ROWS):\n",
    "        for col in range(GRID_COLS):\n",
    "            action_probs = softmax(theta[row, col])\n",
    "            max_action = np.argmax(action_probs)\n",
    "            direction_grid[row, col] = action_symbols[max_action]  # Direction with highest prob\n",
    "            probability_grid[row, col] = np.max(action_probs)      # Highest probability\n",
    "\n",
    "    # Display both grids side by side\n",
    "    print(\"Policy (Direction) | Probabilities (Max)\")\n",
    "    for row in range(GRID_ROWS):\n",
    "        direction_row = \" \".join(direction_grid[row])\n",
    "        probability_row = \" \".join(f\"{prob:.2f}\" for prob in probability_grid[row])\n",
    "        print(f\"{direction_row}      | {probability_row}\")\n",
    "\n",
    "# Policy Gradient algorithm implementation\n",
    "def policy_gradient(grid_iterations=100):\n",
    "    for iteration in range(grid_iterations):\n",
    "        state = (0, 0)  # Start state\n",
    "        episode = []  # Store state, action, reward\n",
    "\n",
    "        while state != (3, 4):  # Run until we reach the goal (bottom-right corner)\n",
    "            action = choose_action(state)\n",
    "            new_state, reward = take_step(state, action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = new_state\n",
    "\n",
    "        # Calculate the return G (sum of discounted rewards)\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = reward + gamma * G  # G ← r_t + γ * G\n",
    "\n",
    "            # Update policy parameters based on the return G\n",
    "            policy_gradient_update(state, action, G)\n",
    "\n",
    "        # Every 10 iterations, print the policy and probabilities\n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            print(f\"Iteration {iteration + 1}:\")\n",
    "            display_policy()\n",
    "            print(\"\\n\")\n",
    "\n",
    "    # Final grid and path taken after training\n",
    "    print(\"Final policy parameters (theta):\")\n",
    "    display_policy()\n",
    "\n",
    "    # Show final path from start to goal\n",
    "    state = (0, 0)\n",
    "    path_taken = [state]\n",
    "    while state != (3, 4):\n",
    "        action = choose_action(state)\n",
    "        state, _ = take_step(state, action)\n",
    "        path_taken.append(state)\n",
    "\n",
    "    print(\"Path taken:\", path_taken)\n",
    "\n",
    "# Call the function to run the policy gradient algorithm\n",
    "policy_gradient(100)  # Run for 100 iterations\n",
    "policy_gradient(100)  # Run for 100 iterations\n",
    "policy_gradient(100)  # Run for 100 iterations\n",
    "policy_gradient(100)  # Run for 100 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Aspects:\n",
    "\n",
    "### 1. Softmax Function:\n",
    "The policy is derived by converting policy parameters `theta` to action probabilities using the softmax function:\n",
    "\n",
    "\\[\n",
    "\\pi(a|s; \\theta) = \\frac{e^{\\theta_{s,a}}}{\\sum_{b} e^{\\theta_{s,b}}}\n",
    "\\]\n",
    "\n",
    "### 2. Action Selection:\n",
    "For each state `(row, col)`, the agent chooses an action based on the probabilities from the softmax function.\n",
    "\n",
    "### 3. Policy Update:\n",
    "The policy is updated using the gradient ascent rule for policy gradient:\n",
    "\n",
    "\\[\n",
    "\\theta_{s,a} \\leftarrow \\theta_{s,a} + \\alpha \\cdot G \\cdot \\nabla \\log \\pi(a|s; \\theta)\n",
    "\\]\n",
    "\n",
    "### 4. Return Calculation (G):\n",
    "The return `G` is computed as the discounted sum of future rewards:\n",
    "\n",
    "\\[\n",
    "G_t = r_t + \\gamma G_{t+1}\n",
    "\\]\n",
    "\n",
    "### 5. Grid and Path Printing:\n",
    "The grid updates are printed every 10 steps to show progress. The final path taken by the agent after training is also printed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
